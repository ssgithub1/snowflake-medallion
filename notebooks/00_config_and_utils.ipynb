{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: 00_config_and_utils\n",
    "# Purpose: Shared configuration helpers and utilities for Snowflake Medallion notebooks.\n",
    "\n",
    "import os\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_env_or_default(name: str, default: str) -> str:\n",
    "    value = os.getenv(name)\n",
    "    return value if value is not None and value != \"\" else default\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Editable parameters (override via env vars if present)\n",
    "# ------------------------------\n",
    "DATASET_NAME = _get_env_or_default(\"DATASET_NAME\", \"nyc_taxi_yellow\")\n",
    "SOURCE_FORMAT = _get_env_or_default(\"SOURCE_FORMAT\", \"parquet\")  # parquet|csv\n",
    "SOURCE_TYPE = _get_env_or_default(\"SOURCE_TYPE\", \"stage\")  # stage|table\n",
    "SOURCE_STAGE = _get_env_or_default(\"SOURCE_STAGE\", \"@RAW_NYC_TAXI\")\n",
    "SOURCE_PATH = _get_env_or_default(\"SOURCE_PATH\", \"yellow/\")\n",
    "SOURCE_TABLE = _get_env_or_default(\"SOURCE_TABLE\", \"\")\n",
    "DATABASE = _get_env_or_default(\"DATABASE\", \"\")\n",
    "SCHEMA = _get_env_or_default(\"SCHEMA\", \"\")\n",
    "BRONZE_TABLE = _get_env_or_default(\"BRONZE_TABLE\", \"BRONZE_NYC_TAXI_YELLOW\")\n",
    "SILVER_TABLE = _get_env_or_default(\"SILVER_TABLE\", \"SILVER_NYC_TAXI_YELLOW\")\n",
    "GOLD_TABLE_PREFIX = _get_env_or_default(\"GOLD_TABLE_PREFIX\", \"GOLD_NYC_TAXI_YELLOW\")\n",
    "PRIMARY_KEYS = _get_env_or_default(\n",
    "    \"PRIMARY_KEYS\",\n",
    "    \"VENDORID,TPEP_PICKUP_DATETIME,TPEP_DROPOFF_DATETIME,PULOCATIONID,DOLOCATIONID\",\n",
    ")\n",
    "EVENT_TIME_COLUMN = _get_env_or_default(\"EVENT_TIME_COLUMN\", \"TPEP_PICKUP_DATETIME\")\n",
    "DEDUPE_ORDER_COLUMN = _get_env_or_default(\"DEDUPE_ORDER_COLUMN\", \"_INGEST_TS\")\n",
    "MERGE_MODE = _get_env_or_default(\"MERGE_MODE\", \"append\")  # append|merge\n",
    "WAREHOUSE = _get_env_or_default(\"WAREHOUSE\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_list(value: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse a CSV string into a list of uppercase, trimmed values.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return []\n",
    "    return [item.strip().upper() for item in value.split(\",\") if item.strip()]\n",
    "\n",
    "\n",
    "def qualify_name(database: str, schema: str, object_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Qualify object name with optional database and schema.\n",
    "    \"\"\"\n",
    "    parts = [p for p in [database, schema, object_name] if p]\n",
    "    return \".\".join(parts)\n",
    "\n",
    "\n",
    "def ensure_db_schema(session, database: str, schema: str) -> None:\n",
    "    \"\"\"\n",
    "    Use database/schema if provided.\n",
    "    \"\"\"\n",
    "    if database:\n",
    "        session.sql(f\"USE DATABASE {database}\").collect()\n",
    "    if schema:\n",
    "        session.sql(f\"USE SCHEMA {schema}\").collect()\n",
    "\n",
    "\n",
    "def ensure_table(session, full_table_name: str, ddl_sql: str) -> None:\n",
    "    \"\"\"\n",
    "    Create table if not exists using provided DDL.\n",
    "    \"\"\"\n",
    "    session.sql(ddl_sql.format(table_name=full_table_name)).collect()\n",
    "\n",
    "\n",
    "def require_non_null(session, full_table_name: str, cols: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Raise ValueError if any columns contain NULL values.\n",
    "    \"\"\"\n",
    "    if not cols:\n",
    "        return\n",
    "    checks = \", \".join([\n",
    "        f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) AS {col}_nulls\" for col in cols\n",
    "    ])\n",
    "    result = session.sql(f\"SELECT {checks} FROM {full_table_name}\").collect()[0].as_dict()\n",
    "    nulls = {k: v for k, v in result.items() if v and int(v) > 0}\n",
    "    if nulls:\n",
    "        details = \", \".join([f\"{k}={v}\" for k, v in nulls.items()])\n",
    "        raise ValueError(f\"Null values found in required columns: {details}\")\n",
    "\n",
    "\n",
    "def dedupe_latest_sql(src_table: str, keys: List[str], order_col: str) -> str:\n",
    "    \"\"\"\n",
    "    Return SQL for deduplicating to latest record per key.\n",
    "    \"\"\"\n",
    "    key_expr = \", \".join(keys)\n",
    "    return (\n",
    "        \"SELECT * FROM \"\n",
    "        f\"{src_table} \"\n",
    "        f\"QUALIFY ROW_NUMBER() OVER (PARTITION BY {key_expr} ORDER BY {order_col} DESC)=1\"\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_identifier(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic validation helper for identifiers.\n",
    "    Note: For full escaping, consider quoting identifiers explicitly.\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        raise ValueError(\"Identifier cannot be empty\")\n",
    "    return name\n",
    "\n",
    "\n",
    "# Standard ingestion metadata columns\n",
    "# _INGEST_TS TIMESTAMP_NTZ default CURRENT_TIMESTAMP()\n",
    "# _SOURCE_FILE STRING (best-effort; if unavailable, set NULL)\n",
    "# _BATCH_ID STRING default UUID_STRING()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Snowpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
